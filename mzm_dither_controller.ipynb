{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4aee20d",
   "metadata": {},
   "source": [
    "# åŸºäºæ·±åº¦å­¦ä¹ çš„ MZM å¯¼é¢‘æ³•é—­ç¯æ§åˆ¶ï¼ˆæ•°æ®é›†ç”Ÿæˆ / è®­ç»ƒ / æ¨ç†ï¼‰\n",
    "\n",
    "æœ¬ Notebook çš„ç›®æ ‡ï¼š\n",
    "- åªä½¿ç”¨ PD è¾“å‡ºçš„å¯¼é¢‘ 1f/2f **åŠŸç‡(dBm)**ï¼ˆä¸ä½¿ç”¨ DC å…‰åŠŸç‡/ä¸ä¾èµ–é«˜é€Ÿ RF å¯è§‚æµ‹ï¼‰ã€‚\n",
    "- ç›®æ ‡è§’åº¦èŒƒå›´ï¼š**0â€“180Â°**ï¼ˆå¯¹åº”å•ä¸ªå‘¨æœŸåŠæ³¢ï¼‰ã€‚\n",
    "- å°† **æ•°æ®é›†ç”Ÿæˆ â†’ è®­ç»ƒ â†’ æ¨ç†/å›æ”¾** æ‹†å¼€ï¼Œä¾¿äºå¤ç”¨å…ˆå‰äº§ç‰©ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdf2224",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import importlib\n",
    "import numpy as np\n",
    "\n",
    "# Reusable pipeline module (reload to pick up edits)\n",
    "import mzm.dither_controller as dc\n",
    "dc = importlib.reload(dc)\n",
    "\n",
    "DATASET_PATH = 'artifacts/dither_dataset_dbm_hist.npz'\n",
    "MODEL_PATH = 'artifacts/dither_policy_dbm_hist.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468fcf60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "\n",
    "print(f\"Python: {sys.version}\")\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"âœ… CUDA is available. Device: {torch.cuda.get_device_name(0)}\")\n",
    "elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "    print(\"âœ… MPS (Metal Performance Shaders) is available on macOS.\")\n",
    "else:\n",
    "    print(\"â„¹ï¸ No GPU acceleration detected. Using CPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9637fa9a",
   "metadata": {},
   "source": [
    "## æ•°æ®é›†ç”Ÿæˆ\n",
    "\n",
    "æ•°æ®é›†ä¼šä¿å­˜åˆ° `artifacts/`ï¼Œä¸‹æ¬¡å¯ç›´æ¥åŠ è½½å¤ç”¨ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210e9bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "device_params = dc.DeviceParams()\n",
    "dither_params = dc.DitherParams()\n",
    "\n",
    "# Generate (or overwrite) dataset artifact\n",
    "# å¢å¤§æ ·æœ¬é‡ä»¥è¦†ç›– 0 åº¦é™„è¿‘çš„ç‰©ç†æ­»åŒºï¼ˆçµæ•åº¦ä½åŒºåŸŸï¼‰\n",
    "ds0 = dc.generate_dataset_dbm_hist(\n",
    "    device_params=device_params,\n",
    "    dither_params=dither_params,\n",
    "    n_samples=60000,  # Increased from 20000 to 60000 to improve edge/null-point accuracy\n",
    "    seed=0,\n",
    "    teacher_gain=0.5,\n",
    "    max_step_V=0.2,\n",
    "    accel='auto',\n",
    "    torch_batch=128,\n",
    " )\n",
    "dc.save_dataset(ds0, DATASET_PATH)\n",
    "print('saved dataset:', DATASET_PATH)\n",
    "\n",
    "# If you used CUDA/MPS, PyTorch may keep a caching allocator reserve in VRAM.\n",
    "# This is normal (not a leak). If you want to release it back to the OS:\n",
    "import gc\n",
    "import torch\n",
    "del ds0\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "    torch.mps.empty_cache()\n",
    "\n",
    "ds = dc.load_dataset(DATASET_PATH)\n",
    "print('Xn:', ds['Xn'].shape, 'y:', ds['y'].shape)\n",
    "print('mu:', ds['mu'].shape, 'sigma:', ds['sigma'].shape)\n",
    "print('device_params:', ds['device_params'])\n",
    "print('dither_params:', ds['dither_params'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409841a2",
   "metadata": {},
   "source": [
    "## è®­ç»ƒ\n",
    "\n",
    "è®­ç»ƒè¾“å‡ºä¿å­˜åˆ° `artifacts/dither_policy_dbm_hist.pt`ï¼Œå¯é‡å¤åŠ è½½å¤ç”¨ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430a495a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import mzm.dither_controller as dc\n",
    "dc = importlib.reload(dc)\n",
    "\n",
    "ds = dc.load_dataset(DATASET_PATH)\n",
    "\n",
    "# Note: accel='cpu' is often faster for small models due to lower latency\n",
    "# We have optimized the training loop to remove DataLoader overhead.\n",
    "# è°ƒæ•´è®­ç»ƒå‚æ•°ä»¥é¿å…è¿‡æ‹Ÿåˆå™ªå£°ï¼Œå¹¶æé«˜åœ¨æ­»åŒºé™„è¿‘çš„æ³›åŒ–èƒ½åŠ›\n",
    "model = dc.train_policy(\n",
    "    Xn=ds['Xn'],\n",
    "    y=ds['y'],\n",
    "    epochs=5000,   # é™ä½è½®æ•°ï¼Œé˜²æ­¢å¯¹ 0 åº¦é™„è¿‘çš„å™ªå£°è¿‡æ‹Ÿåˆ (was 10000)\n",
    "    batch=1024,    # å¢å¤§ Batch Sizeï¼Œä½¿æ¢¯åº¦æ›´æ–°æ›´å¹³æ»‘ (was 256)\n",
    "    lr=5e-4,       # é™ä½å­¦ä¹ ç‡ï¼Œç²¾ç»†æœç´¢æœ€ä¼˜è§£ (was 1e-3)\n",
    "    hidden=128,    # å¢åŠ æ¨¡å‹å®¹é‡ï¼Œä»¥æ‹Ÿåˆéçº¿æ€§æå€¼ç‚¹ç‰¹å¾ (was 64)\n",
    "    depth=3,\n",
    "    seed=0,\n",
    "    accel='auto',\n",
    " )\n",
    "\n",
    "dc.save_model(\n",
    "    model=model,\n",
    "    mu=ds['mu'],\n",
    "    sigma=ds['sigma'],\n",
    "    device_params=ds['device_params'],\n",
    "    dither_params=ds['dither_params'],\n",
    "    path=MODEL_PATH,\n",
    " )\n",
    "print('saved model:', MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b3af5e",
   "metadata": {},
   "source": [
    "## æ¨ç†/å›æ”¾ï¼ˆå•ç›®æ ‡è§’åº¦ + é€è½®è¿‡ç¨‹ + å‰åå…‰è°±å¯¹æ¯”ï¼‰\n",
    "\n",
    "- é€‰æ‹©ä¸€ä¸ª `target_deg` ä½œä¸ºé”å®šç›®æ ‡ï¼›\n",
    "- å±•ç¤ºæ¯ä¸€è½®è¿­ä»£çš„è§‚æµ‹ä¸åŠ¨ä½œï¼ˆ`p1/p2`ã€å·®åˆ†ã€`Î”V`ã€`V`ã€`Î¸`ã€è¯¯å·®ï¼‰ï¼›\n",
    "- åœ¨åˆå§‹åç½®ä¸ç»ˆæ­¢åç½®åˆ†åˆ«ç»˜åˆ¶ä¸€æ¬¡**å…‰è°±/ç”µè°±**ç”¨äºå‰åå¯¹æ¯”ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2d7349",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from mzm.model import simulate_mzm, bias_to_theta_rad\n",
    "from mzm.plot import plot_optical_spectrum_osa, plot_electrical_spectrum\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "model, meta = dc.load_model(MODEL_PATH)\n",
    "\n",
    "# Choose a single target working point (deg)\n",
    "target_deg = 0.0\n",
    "\n",
    "# Initial bias: random by default.\n",
    "# - Set init_seed to an integer for reproducible runs (e.g., 1).\n",
    "# - Keep it as None to re-sample a different V_init each run.\n",
    "init_seed = None\n",
    "rng = np.random.default_rng(init_seed)\n",
    "V_init = float(rng.uniform(0.0, float(meta['device_params'].Vpi_DC)))\n",
    "\n",
    "theta_init_deg = float(np.rad2deg(bias_to_theta_rad(V_init, Vpi_DC=float(meta['device_params'].Vpi_DC))))\n",
    "\n",
    "print('target_deg     :', target_deg)\n",
    "print('init V (V)     :', V_init)\n",
    "print('init theta (deg):', theta_init_deg)\n",
    "\n",
    "# --- (A) Pre-control spectra at initial bias ---\n",
    "print('\\n[pre] spectra at V_init')\n",
    "sim_pre = simulate_mzm(\n",
    "    V_bias=V_init,\n",
    "    Vpi_RF=float(meta['device_params'].Vpi_DC),\n",
    "    Vpi_DC=float(meta['device_params'].Vpi_DC),\n",
    "    ER_dB=float(meta['device_params'].ER_dB),\n",
    "    IL_dB=float(meta['device_params'].IL_dB),\n",
    "    Pin_dBm=float(meta['device_params'].Pin_dBm),\n",
    "    Responsivity=float(meta['device_params'].Responsivity),\n",
    "    R_load=float(meta['device_params'].R_load),\n",
    "    # RF settings used only for spectrum illustration\n",
    "    f_rf=1e9,\n",
    "    V_rf_amp=0.2,\n",
    "    Fs=100e9,\n",
    "    T_total=5e-6,\n",
    " )\n",
    "print('pre  P_pd_avg_dBm:', sim_pre.P_pd_avg_dBm)\n",
    "plot_optical_spectrum_osa(sim_pre, f_rf_hz=1e9)\n",
    "plot_electrical_spectrum(sim_pre, f_rf_hz=1e9)\n",
    "\n",
    "# --- (B) Closed-loop rollout (dBm-only + history) ---\n",
    "steps = 60\n",
    "r = dc.rollout_dbm_hist(\n",
    "    model=model,\n",
    "    mu=meta['mu'],\n",
    "    sigma=meta['sigma'],\n",
    "    device_params=meta['device_params'],\n",
    "    dither_params=meta['dither_params'],\n",
    "    theta_target_deg=float(target_deg),\n",
    "    V_init=V_init,\n",
    "    steps=steps,\n",
    "    accel='cpu',\n",
    " )\n",
    "\n",
    "V_final = float(r['V'][-1])\n",
    "final_err = float(r['err_deg'][-1])\n",
    "print('\\n[rollout] final V:', V_final)\n",
    "print('[rollout] final wrapped err (deg):', f\"{final_err:+.2f}\")\n",
    "\n",
    "# Print per-iteration trace\n",
    "print('\\nPer-iteration trace:')\n",
    "print(' iter |    V(V) |  theta(deg) |  err(deg) |   dV(V) |  p1(dBm) |  p2(dBm) | dp1(dB) | dp2(dB)')\n",
    "for k in range(steps):\n",
    "    print(\n",
    "        f\"{k+1:5d} |\"\n",
    "        f\" {r['V'][k]:7.4f} |\"\n",
    "        f\" {r['theta_deg'][k]:10.3f} |\"\n",
    "        f\" {r['err_deg'][k]:8.3f} |\"\n",
    "        f\" {r['dv'][k]:7.4f} |\"\n",
    "        f\" {r['p1_dBm'][k]:8.2f} |\"\n",
    "        f\" {r['p2_dBm'][k]:8.2f} |\"\n",
    "        f\" {r['dp1_dBm'][k]:7.2f} |\"\n",
    "        f\" {r['dp2_dBm'][k]:7.2f}\"\n",
    "    )\n",
    "\n",
    "# Plot convergence curve\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(r['err_deg'], label=f\"target {target_deg:.0f}Â°\")\n",
    "plt.axhline(0.0, color='k', linewidth=1)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Phase Error (deg, wrapped)')\n",
    "plt.title('Closed-loop convergence (dBm-only + history)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- (C) Post-control spectra at final bias ---\n",
    "print('\\n[post] spectra at V_final')\n",
    "sim_post = simulate_mzm(\n",
    "    V_bias=V_final,\n",
    "    Vpi_RF=float(meta['device_params'].Vpi_DC),\n",
    "    Vpi_DC=float(meta['device_params'].Vpi_DC),\n",
    "    ER_dB=float(meta['device_params'].ER_dB),\n",
    "    IL_dB=float(meta['device_params'].IL_dB),\n",
    "    Pin_dBm=float(meta['device_params'].Pin_dBm),\n",
    "    Responsivity=float(meta['device_params'].Responsivity),\n",
    "    R_load=float(meta['device_params'].R_load),\n",
    "    f_rf=1e9,\n",
    "    V_rf_amp=0.2,\n",
    "    Fs=100e9,\n",
    "    T_total=5e-6,\n",
    " )\n",
    "print('post P_pd_avg_dBm:', sim_post.P_pd_avg_dBm)\n",
    "plot_optical_spectrum_osa(sim_post, f_rf_hz=1e9)\n",
    "plot_electrical_spectrum(sim_post, f_rf_hz=1e9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27db18a5",
   "metadata": {},
   "source": [
    "## æ¨¡å‹è¯„ä¼°ä¸å¯¹æ¯”\n",
    "\n",
    "æ‰«æ 0-180Â° ç›®æ ‡è§’åº¦ï¼Œæ¯ä¸ªè§’åº¦é‡å¤æ¨ç† 50 æ¬¡ï¼ˆæ ¹æ®æ˜¾å­˜å¤§å°è°ƒæ•´æ¨ç†æ¬¡æ•°ï¼‰ã€‚\n",
    "- **å¯è§†åŒ–**ï¼šç»˜åˆ¶å¹³å‡è¯¯å·®æ›²çº¿ï¼Œå¹¶ä½¿ç”¨é˜´å½±åŒºåŸŸè¡¨ç¤ºæ–¹å·®ï¼ˆæ ‡å‡†å·®èŒƒå›´ï¼‰ï¼Œç±»ä¼¼ K çº¿å›¾çš„æ¦‚å¿µå±•ç¤ºæ³¢åŠ¨æƒ…å†µã€‚\n",
    "- **ç¼“å­˜**ï¼šè‡ªåŠ¨åŠ è½½å†å²æœ€ä½³æ¨¡å‹çš„è¯„ä¼°ç»“æœï¼ˆ`artifacts/dither_policy_dbm_hist_best_eval.npz`ï¼‰ï¼Œé¿å…é‡å¤è®¡ç®—ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d64697",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import hashlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import importlib\n",
    "import gc\n",
    "import mzm.dither_controller as dc\n",
    "\n",
    "# Reload to ensure we have the new batch function\n",
    "dc = importlib.reload(dc)\n",
    "\n",
    "# Paths\n",
    "BEST_MODEL_PATH = 'artifacts/dither_policy_dbm_hist_best.pt'\n",
    "BEST_EVAL_PATH = 'artifacts/dither_policy_dbm_hist_best_eval.npz'\n",
    "\n",
    "def get_file_hash(filepath):\n",
    "    \"\"\"Calculate MD5 hash of a file.\"\"\"\n",
    "    if not os.path.exists(filepath):\n",
    "        return None\n",
    "    with open(filepath, \"rb\") as f:\n",
    "        return hashlib.md5(f.read()).hexdigest()\n",
    "\n",
    "def evaluate_model(model_path, label, steps=60, n_repeats=50):\n",
    "    print(f\"Evaluating model: {label} ...\")\n",
    "    \n",
    "    # Load model\n",
    "    try:\n",
    "        loaded_model, loaded_meta = dc.load_model(model_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Model not found: {model_path}\")\n",
    "        return None\n",
    "\n",
    "    targets = np.arange(0, 181, 1)\n",
    "    \n",
    "    # Use the params from the loaded model\n",
    "    device_params = loaded_meta['device_params']\n",
    "    dither_params = loaded_meta['dither_params']\n",
    "    mu = loaded_meta['mu']\n",
    "    sigma = loaded_meta['sigma']\n",
    "    \n",
    "    init_seed = None\n",
    "    rng = np.random.default_rng(init_seed)\n",
    "    \n",
    "    # Prepare Batch Inputs\n",
    "    # Repeat targets: [0, 0, ..., 1, 1, ..., 180, 180, ...]\n",
    "    targets_repeated = np.repeat(targets, n_repeats)\n",
    "    n_total = len(targets_repeated)\n",
    "    \n",
    "    # Random V_init for each simulation\n",
    "    V_init_batch = rng.uniform(0.0, float(device_params.Vpi_DC), size=n_total)\n",
    "    \n",
    "    # print(f\"Running batch inference for {n_total} trajectories...\")\n",
    "    \n",
    "    # Run batch rollout\n",
    "    # Note: We use 'auto' here because batch size is large (~1800), so GPU/MPS acceleration is beneficial.\n",
    "    r = dc.rollout_dbm_hist_batch(\n",
    "        model=loaded_model,\n",
    "        mu=mu,\n",
    "        sigma=sigma,\n",
    "        device_params=device_params,\n",
    "        dither_params=dither_params,\n",
    "        theta_target_deg=targets_repeated,\n",
    "        V_init=V_init_batch,\n",
    "        steps=steps,\n",
    "        accel='auto' \n",
    "    )\n",
    "    \n",
    "    # Extract final errors (absolute)\n",
    "    # r['err_deg'] shape is (N, steps)\n",
    "    final_errs = np.abs(r['err_deg'][:, -1])\n",
    "    \n",
    "    # Cleanup memory immediately\n",
    "    del r\n",
    "    del loaded_model\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "        torch.mps.empty_cache()\n",
    "    \n",
    "    # Reshape to (n_targets, n_repeats)\n",
    "    final_errs_reshaped = final_errs.reshape(len(targets), n_repeats)\n",
    "    \n",
    "    # Calculate stats\n",
    "    means = np.mean(final_errs_reshaped, axis=1)\n",
    "    stds = np.std(final_errs_reshaped, axis=1)\n",
    "    mins = np.min(final_errs_reshaped, axis=1)\n",
    "    maxs = np.max(final_errs_reshaped, axis=1)\n",
    "        \n",
    "    return {\n",
    "        'targets': targets,\n",
    "        'mean': means,\n",
    "        'std': stds,\n",
    "        'min': mins,\n",
    "        'max': maxs,\n",
    "        'label': label\n",
    "    }\n",
    "\n",
    "def plot_eval_results(current_stats, best_stats=None):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Helper to plot one set of stats\n",
    "    def plot_one(stats, color, label_prefix):\n",
    "        t = stats['targets']\n",
    "        m = stats['mean']\n",
    "        s = stats['std']\n",
    "        \n",
    "        # Plot Mean\n",
    "        plt.plot(t, m, color=color, label=f\"{label_prefix} Mean MAE\", linewidth=2)\n",
    "        \n",
    "        # Plot Std Dev (Variance representation)\n",
    "        # Clip lower bound to 0 because absolute error cannot be negative\n",
    "        lower_bound = np.maximum(m - s, 0)\n",
    "        plt.fill_between(t, lower_bound, m + s, color=color, alpha=0.2, label=f\"{label_prefix} Â±1 Std (Variance)\")\n",
    "\n",
    "    if current_stats:\n",
    "        plot_one(current_stats, 'blue', 'Current')\n",
    "        print(f\"Current Model Overall Mean MAE: {np.mean(current_stats['mean']):.4f} deg\")\n",
    "\n",
    "    if best_stats:\n",
    "        plot_one(best_stats, 'orange', 'Best')\n",
    "        print(f\"Best Model Overall Mean MAE: {np.mean(best_stats['mean']):.4f} deg\")\n",
    "\n",
    "    plt.xlabel('Target Angle (deg)')\n",
    "    plt.ylabel('Absolute Error (deg)')\n",
    "    plt.title('Model Evaluation: Error Distribution vs Target Angle (0-180Â°)')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "# 1. Evaluate Current Model\n",
    "current_stats = evaluate_model(MODEL_PATH, \"Current Model\")\n",
    "\n",
    "# 2. Load Best Stats (if available) or Evaluate Best Model (if pt exists but npz doesn't)\n",
    "best_stats = None\n",
    "\n",
    "if os.path.exists(BEST_EVAL_PATH):\n",
    "    print(f\"Loading cached best evaluation from {BEST_EVAL_PATH}\")\n",
    "    data = np.load(BEST_EVAL_PATH)\n",
    "    best_stats = {k: data[k] for k in data.files}\n",
    "    best_stats['label'] = \"Best Model (Cached)\"\n",
    "elif os.path.exists(BEST_MODEL_PATH):\n",
    "    print(\"Cached evaluation not found, re-evaluating best model...\")\n",
    "    best_stats = evaluate_model(BEST_MODEL_PATH, \"Best Model\")\n",
    "\n",
    "# 3. Plot\n",
    "plot_eval_results(current_stats, best_stats)\n",
    "\n",
    "# 4. Automatic Update Logic\n",
    "if current_stats:\n",
    "    curr_mae = np.mean(current_stats['mean'])\n",
    "    \n",
    "    if best_stats:\n",
    "        best_mae = np.mean(best_stats['mean'])\n",
    "        print(f\"\\nComparison: Current ({curr_mae:.4f}) vs Best ({best_mae:.4f})\")\n",
    "        \n",
    "        if curr_mae < best_mae:\n",
    "            # Check if models are identical\n",
    "            curr_hash = get_file_hash(MODEL_PATH)\n",
    "            best_hash = get_file_hash(BEST_MODEL_PATH)\n",
    "            \n",
    "            if curr_hash == best_hash:\n",
    "                print(\"âš ï¸ Current model file is identical to the best, but metrics improved. Updating best evaluation cache only.\")\n",
    "                np.savez(\n",
    "                    BEST_EVAL_PATH,\n",
    "                    targets=current_stats['targets'],\n",
    "                    mean=current_stats['mean'],\n",
    "                    std=current_stats['std'],\n",
    "                    min=current_stats['min'],\n",
    "                    max=current_stats['max']\n",
    "                )\n",
    "                print(f\"âœ… Updated best stats: {BEST_EVAL_PATH}\")\n",
    "            else:\n",
    "                print(\"ğŸš€ Current model is BETTER! Updating best model...\")\n",
    "                shutil.copy2(MODEL_PATH, BEST_MODEL_PATH)\n",
    "                np.savez(\n",
    "                    BEST_EVAL_PATH,\n",
    "                    targets=current_stats['targets'],\n",
    "                    mean=current_stats['mean'],\n",
    "                    std=current_stats['std'],\n",
    "                    min=current_stats['min'],\n",
    "                    max=current_stats['max']\n",
    "                )\n",
    "                print(f\"âœ… Updated best model: {BEST_MODEL_PATH}\")\n",
    "                print(f\"âœ… Updated best stats: {BEST_EVAL_PATH}\")\n",
    "        else:\n",
    "            print(\"ğŸ“‰ Current model is NOT better than the best model. Keep training!\")\n",
    "    else:\n",
    "        # No best model exists yet\n",
    "        print(\"ğŸš€ No best model found. Setting current model as best...\")\n",
    "        shutil.copy2(MODEL_PATH, BEST_MODEL_PATH)\n",
    "        np.savez(\n",
    "            BEST_EVAL_PATH,\n",
    "            targets=current_stats['targets'],\n",
    "            mean=current_stats['mean'],\n",
    "            std=current_stats['std'],\n",
    "            min=current_stats['min'],\n",
    "            max=current_stats['max']\n",
    "        )\n",
    "        print(f\"âœ… Created best model: {BEST_MODEL_PATH}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
